{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a60bffde",
   "metadata": {},
   "source": [
    "# HW02: Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ed3f1a",
   "metadata": {},
   "source": [
    "Remember that these homework work as a completion grade. **You can skip one section without losing credit.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc19cf83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>lead</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>Stocks End Up, But Near Year Lows (Reuters)</td>\n",
       "      <td>Reuters - Stocks ended slightly higher on Frid...</td>\n",
       "      <td>Stocks End Up, But Near Year Lows (Reuters) Re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                              title  \\\n",
       "0  business  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "1  business    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "2  business  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "3  business  Oil prices soar to all-time record, posing new...   \n",
       "4  business        Stocks End Up, But Near Year Lows (Reuters)   \n",
       "\n",
       "                                                lead  \\\n",
       "0  Reuters - Private investment firm Carlyle Grou...   \n",
       "1  Reuters - Soaring crude prices plus worries\\ab...   \n",
       "2  Reuters - Authorities have halted oil export\\f...   \n",
       "3  AFP - Tearaway world oil prices, toppling reco...   \n",
       "4  Reuters - Stocks ended slightly higher on Frid...   \n",
       "\n",
       "                                                text  \n",
       "0  Carlyle Looks Toward Commercial Aerospace (Reu...  \n",
       "1  Oil and Economy Cloud Stocks' Outlook (Reuters...  \n",
       "2  Iraq Halts Oil Exports from Main Southern Pipe...  \n",
       "3  Oil prices soar to all-time record, posing new...  \n",
       "4  Stocks End Up, But Near Year Lows (Reuters) Re...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import the AG news dataset (same as hw01)\n",
    "#Download them from here \n",
    "#!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df.columns = [\"label\", \"title\", \"lead\"]\n",
    "label_map = {1:\"world\", 2:\"sport\", 3:\"business\", 4:\"sci/tech\"}\n",
    "def replace_label(x):\n",
    "\treturn label_map[x]\n",
    "df[\"label\"] = df[\"label\"].apply(replace_label) \n",
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"lead\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f7b626",
   "metadata": {},
   "source": [
    "## Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab1861a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eagles' Biggest Hurdle Lies Ahead EAST RUTHERFORD, N.J., Nov. 28 - Even the flamboyant Terrell Owens was subdued after Philadelphia's 27-6 victory over the Giants on Sunday.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "dfs = df.sample(50)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "##TODO use spacy to split the documents in the sampled dataframe (dfs) in sentences and tokens\n",
    "dfs['document'] = df['title'] + ' ' + df['lead'] + ' ' + df['text']\n",
    "    # above I created a new column like in hw_01 where all the text from these columns is combined.\n",
    "\n",
    "# For storing sentences and tokens\n",
    "sentences = list()\n",
    "tokens = list()\n",
    "\n",
    "for doc_text in dfs['document']:\n",
    "    doc = nlp(doc_text)\n",
    "\n",
    "    sentences.extend([sentence.text for sentence in doc.sents])\n",
    "    tokens.extend([token.text for token in doc])\n",
    "\n",
    "##TODO print the first sentence of the first document in your sample\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65fa4c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eagles eagle poss\n",
      "' ' case\n",
      "Biggest big amod\n",
      "Hurdle Hurdle nsubj\n",
      "Lies lie ROOT\n",
      "Ahead ahead advmod\n",
      "EAST EAST compound\n",
      "RUTHERFORD RUTHERFORD nsubjpass\n",
      ", , punct\n",
      "N.J. N.J. appos\n",
      ", , punct\n",
      "Nov. Nov. npadvmod\n",
      "28 28 nummod\n",
      "- - punct\n",
      "Even even advmod\n",
      "the the det\n",
      "flamboyant flamboyant amod\n",
      "Terrell Terrell compound\n",
      "Owens Owens nsubjpass\n",
      "was be auxpass\n",
      "subdued subdue advcl\n",
      "after after prep\n",
      "Philadelphia Philadelphia poss\n",
      "'s 's case\n",
      "27 27 nummod\n",
      "- - punct\n",
      "6 6 prep\n",
      "victory victory pobj\n",
      "over over prep\n",
      "the the det\n",
      "Giants Giants pobj\n",
      "on on prep\n",
      "Sunday Sunday pobj\n",
      ". . punct\n"
     ]
    }
   ],
   "source": [
    "##TODO create a new column with tokens in lowercase (x.lower()), without punctuation tokens (x.is_punct), stopwords (x.is_stop), and digits (x.is_digit)\n",
    "# dfs['processed'] = dfs['document'].apply(lambda doc: [token.text.lower() for token in nlp(doc) if not token.is_punct and not token.is_stop and not token.is_digit])\n",
    "\n",
    "\n",
    "##TODO print the tokens (x.lemma_) and the dependency labels (x.dep_ ) of the first sentence of the first document (doc.sents)\n",
    "\n",
    "for token in nlp(sentences[0]):\n",
    "    print(token, token.lemma_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0294d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103735    [eagles, biggest, hurdle, lies, ahead, east, r...\n",
       "97576     [robinson, hails, hodgson, heroics, charlie, h...\n",
       "55421     [honor, system, flu, shots, u.s., chain, store...\n",
       "10214     [iraq, sistani, begins, journey, najaf, witnes...\n",
       "91886     [angels, renew, molina, ortiz, cbc, sports, on...\n",
       "Name: processed, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['processed'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef544f1",
   "metadata": {},
   "source": [
    "### Named Entities\n",
    "\n",
    "Let's compute the ratio of named entities starting with a capital letter, e.g. if we have \"University of Chicago\" as a NE, \"University\" and \"Chicago\" are capitalized, \"of\" is not, thus the ratio is 2/3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be2cb111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of capitalised NEs to the total number of NEs: 427/617\n"
     ]
    }
   ],
   "source": [
    "##TODO print the ratio of tokens being part of a named entity span starting with a capital letter (doc.ents)\n",
    "total_ne = 0\n",
    "cap_ne = 0\n",
    "\n",
    "for doc_text in dfs['document']:\n",
    "    doc = nlp(doc_text)\n",
    "\n",
    "    # now identify all NEs in doc\n",
    "    for ent in doc.ents:\n",
    "        #keep track of number of all entitites\n",
    "        total_ne += 1\n",
    "        if ent.text[0].isupper(): #check if entity is capitalised\n",
    "            cap_ne += 1\n",
    "\n",
    "print('Ratio of capitalised NEs to the total number of NEs: {}/{}'.format(cap_ne, total_ne))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "984b4d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of capitalised non-NEs to the total number of tokens: 336/4566\n"
     ]
    }
   ],
   "source": [
    "##TODO print the ratio of capitalized tokens not being part of a named entity span (have no token.ent_type_)\n",
    "# e.g. \"The dog barks\" = 1/3; 3 tokens, only \"The\" is capitalized\n",
    "\n",
    "total_tok = 0\n",
    "cap_tok = 0\n",
    "\n",
    "for doc_text in dfs['document']:\n",
    "    doc = nlp(doc_text)\n",
    "    total_tok += len(doc) #Count total number of tokens per doc\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.ent_type_ == \"\" and token.text[0].isupper():\n",
    "            cap_tok += 1\n",
    "\n",
    "print('Ratio of capitalised non-NEs to the total number of tokens: {}/{}'.format(cap_tok, total_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac3ede3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of capitalised non-NEs/non-start tokens to the total number of tokens: 263/4566\n"
     ]
    }
   ],
   "source": [
    "##TODO print the ratio of capitalized tokens not being a named entity and not being the first token in a sentence\n",
    "# e.g. \"The dog barks\" = 0; 3 tokens, \"The\" is capitalized but the starting token of a sentence, no other tokens are capitalized.\n",
    "total_tok = 0\n",
    "cap_tok = 0\n",
    "\n",
    "for doc_text in dfs['document']:\n",
    "    doc = nlp(doc_text)\n",
    "    total_tok += len(doc) #Count total number of tokens per doc\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.ent_type_ == \"\" and not token.is_sent_start and token.text[0].isupper():\n",
    "            cap_tok += 1\n",
    "\n",
    "print('Ratio of capitalised non-NEs/non-start tokens to the total number of tokens: {}/{}'.format(cap_tok, total_tok))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466753be",
   "metadata": {},
   "source": [
    "Give an example of a capitalized token in the data which is neither a named entity nor at the start of a sentence. What could be the reason the token is capitalized (one sentence)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e21c0a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PITTSBURGH\n"
     ]
    }
   ],
   "source": [
    "found_example = False\n",
    "for doc_text in dfs['document']:\n",
    "    doc = nlp(doc_text)\n",
    "    for token in doc:\n",
    "        if not found_example and token.ent_type_ == \"\" and not token.is_sent_start and token.text[0].isupper():\n",
    "            print(token.text)\n",
    "            found_example = True\n",
    "            break\n",
    "    if found_example:\n",
    "        break\n",
    "    \n",
    "\n",
    "# These tokens could be capitalised because of stylistic choices perhaps, for example they are part of a news headline which often capitalises most nouns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3126a5c7",
   "metadata": {},
   "source": [
    "## Term Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4369300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gm',\n",
       " 'close',\n",
       " 'baltimore',\n",
       " 'plant',\n",
       " 'general',\n",
       " 'motors',\n",
       " 'corp.',\n",
       " 'world',\n",
       " '39;s',\n",
       " 'largest',\n",
       " 'automaker',\n",
       " 'announced',\n",
       " 'tuesday',\n",
       " '1,100',\n",
       " 'remaining',\n",
       " 'workers',\n",
       " 'baltimore',\n",
       " 'assembly',\n",
       " 'plant',\n",
       " 'lose',\n",
       " 'jobs',\n",
       " 'year',\n",
       " ' ',\n",
       " 'gm',\n",
       " 'close',\n",
       " 'baltimore',\n",
       " 'plant',\n",
       " 'general',\n",
       " 'motors',\n",
       " 'corp.',\n",
       " 'world',\n",
       " '39;s',\n",
       " 'largest',\n",
       " 'automaker',\n",
       " 'announced',\n",
       " 'tuesday',\n",
       " '1,100',\n",
       " 'remaining',\n",
       " 'workers',\n",
       " 'baltimore',\n",
       " 'assembly',\n",
       " 'plant',\n",
       " 'lose',\n",
       " 'jobs',\n",
       " 'year']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business_text = dfs[dfs['label'] == 'business']['document'].apply.tolist()\n",
    "business_text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "055edce8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m##TODO using the whole sample, produce a world cloud with bigrams for label == business using tfidf frequencies\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43mtfidf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbusiness_text\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m tfidf\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n\u001b[0;32m     17\u001b[0m tfidf_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(feature_names, tfidf\u001b[38;5;241m.\u001b[39midf_))\n",
      "File \u001b[1;32mc:\\Users\\manth\\.conda\\envs\\lm_lss\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\manth\\.conda\\envs\\lm_lss\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2096\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2089\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_for_unused_params()\n\u001b[0;32m   2090\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2091\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2092\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2093\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2094\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2095\u001b[0m )\n\u001b[1;32m-> 2096\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2098\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\manth\\.conda\\envs\\lm_lss\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\manth\\.conda\\envs\\lm_lss\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1383\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1375\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1376\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1377\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1378\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1379\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1380\u001b[0m             )\n\u001b[0;32m   1381\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1383\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1386\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\manth\\.conda\\envs\\lm_lss\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1270\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1269\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1270\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1271\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1272\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\manth\\.conda\\envs\\lm_lss\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:110\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32mc:\\Users\\manth\\.conda\\envs\\lm_lss\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:68\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[1;32m---> 68\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(min_df=0.01, \n",
    "                        max_df=0.9,  \n",
    "                        max_features=1000,\n",
    "                        stop_words='english',\n",
    "                        use_idf=True, # the new piece\n",
    "                        ngram_range=(1,2))\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##TODO using the whole sample, produce a world cloud with bigrams for label == business using tfidf frequencies\n",
    "tfidf.fit([business_text])\n",
    "\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "tfidf_dict = dict(zip(feature_names, tfidf.idf_))\n",
    "\n",
    "# Generate word cloud using TF-IDF scores\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(tfidf_dict)\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud for \"Business\" Label with Bigrams (TF-IDF)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "baf87028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'short lived bush rally washing'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business_text[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0948e50f",
   "metadata": {},
   "source": [
    "## Supervised Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0510aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "\n",
    "##TODO compute the number of words per document (excluding stopwords)\n",
    "##TODO get the most predictive features of the number of words per document using first f_class and then chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaee2e4",
   "metadata": {},
   "source": [
    "Are the results different? What could be a reason for this? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af74a1d",
   "metadata": {},
   "source": [
    "## Huggingface Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e75e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we use distilbert tokenizer\n",
    "# !pip install transformers\n",
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "# let's instantiate a tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "##TODO tokenize the sentences in the sampled dataframe (dfs) using the DisilBertTokenizer\n",
    "##TODO what is the type/token ratio from this tokenizer (number_of_unqiue_token_types/number_of_tokens)?\n",
    "##TODO what is the amount of subword tokens returned by the huggingface tokenizer? hint: each subword token starts with \"#\"\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38bc9d00",
   "metadata": {},
   "source": [
    "# Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16bf8639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>lead</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>110581</th>\n",
       "      <td>sport</td>\n",
       "      <td>UEFA to probe Valencia-Werder Bremen incidents</td>\n",
       "      <td>UEFA will be launching disciplinary proceeding...</td>\n",
       "      <td>UEFA to probe Valencia-Werder Bremen incidents...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15925</th>\n",
       "      <td>sci/tech</td>\n",
       "      <td>New iMac tries to play it cool</td>\n",
       "      <td>Hot G5 chip requires some serious effort to av...</td>\n",
       "      <td>New iMac tries to play it cool Hot G5 chip req...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46720</th>\n",
       "      <td>business</td>\n",
       "      <td>Ford Reports Disappointing U.S. Sales</td>\n",
       "      <td>Ford's car and truck business sales fell nearl...</td>\n",
       "      <td>Ford Reports Disappointing U.S. Sales Ford's c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57350</th>\n",
       "      <td>sci/tech</td>\n",
       "      <td>Microsoft Upgrades Windows XP Media Center (Ne...</td>\n",
       "      <td>NewsFactor - Bill Gates is about to announce a...</td>\n",
       "      <td>Microsoft Upgrades Windows XP Media Center (Ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90601</th>\n",
       "      <td>sport</td>\n",
       "      <td>Rams make statement they #39;re team to beat i...</td>\n",
       "      <td>ST. LOUIS -- When St. Louis coach Mike Martz l...</td>\n",
       "      <td>Rams make statement they #39;re team to beat i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           label                                              title  \\\n",
       "110581     sport     UEFA to probe Valencia-Werder Bremen incidents   \n",
       "15925   sci/tech                     New iMac tries to play it cool   \n",
       "46720   business              Ford Reports Disappointing U.S. Sales   \n",
       "57350   sci/tech  Microsoft Upgrades Windows XP Media Center (Ne...   \n",
       "90601      sport  Rams make statement they #39;re team to beat i...   \n",
       "\n",
       "                                                     lead  \\\n",
       "110581  UEFA will be launching disciplinary proceeding...   \n",
       "15925   Hot G5 chip requires some serious effort to av...   \n",
       "46720   Ford's car and truck business sales fell nearl...   \n",
       "57350   NewsFactor - Bill Gates is about to announce a...   \n",
       "90601   ST. LOUIS -- When St. Louis coach Mike Martz l...   \n",
       "\n",
       "                                                     text  \n",
       "110581  UEFA to probe Valencia-Werder Bremen incidents...  \n",
       "15925   New iMac tries to play it cool Hot G5 chip req...  \n",
       "46720   Ford Reports Disappointing U.S. Sales Ford's c...  \n",
       "57350   Microsoft Upgrades Windows XP Media Center (Ne...  \n",
       "90601   Rams make statement they #39;re team to beat i...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df.columns = [\"label\", \"title\", \"lead\"]\n",
    "label_map = {1:\"world\", 2:\"sport\", 3:\"business\", 4:\"sci/tech\"}\n",
    "def replace_label(x):\n",
    "\treturn label_map[x]\n",
    "df[\"label\"] = df[\"label\"].apply(replace_label) \n",
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"lead\"]\n",
    "df = df.sample(n=10000) # # only use 10K datapoints\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61ec5921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "#TODO preprocess the corpus using spacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0ae310",
   "metadata": {},
   "source": [
    "### Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbd7718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subject_verb_pairs(sent):\n",
    "    subjs = [w for w in sent if w.dep_ == \"nsubj\"]\n",
    "    pairs = [(w.lemma_.lower(), w.head.lemma_.lower()) for w in subjs]\n",
    "    return pairs\n",
    "##TODO extract the subject-verbs pairs and print the result for the second document\n",
    "\n",
    "from collections import Counter\n",
    "counter = Counter()\n",
    "\n",
    "##TODO create a list ranking the most common pairs and print the first 10 items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c30e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO do the same for verbs-object pairs ('dobj')\n",
    "##TODO create a list ranking the most common pairs and print the first 10 items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad55039",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO do the same for adjectives-nouns pairs ('amod')\n",
    "##TODO create a list ranking the most common pairs and print the first 10 items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1273253",
   "metadata": {},
   "source": [
    "### Exploring cross label dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d19500",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO extract all the subject-verbs and verbs-object pairs for the verb \"rise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75649d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO for each label create a list ranking the most common subject-verbs pairs and one for the most common verbs-object pairs\n",
    "##TODO print the 10 most common pairs for each of the two lists for the labels \"world\" and \"sci/tech\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98d363b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.system('jupyter nbconvert --to html homework_02.ipynb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "d73fbafa1c1ce4c8cd6f07277ecbf2d62d7720c8bfdb6bdbaeeaf6dba6dd25dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
